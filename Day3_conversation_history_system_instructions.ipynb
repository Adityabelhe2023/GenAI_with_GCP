{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ff303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-goog-api-key\": gemini_api_key\n",
    "}\n",
    "\n",
    "def get_ai_response(user_message, max_tokens=1500, temperature=0.7):\n",
    "    \"\"\"Simple function to get AI response from Gemini API\"\"\"\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"text\": user_message\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxOutputTokens\": max_tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response_data = response.json()\n",
    "    \n",
    "    if 'candidates' in response_data:\n",
    "        return response_data['candidates'][0]['content']['parts'][0]['text'].strip()\n",
    "    else:\n",
    "        return \"Error: No response generated\"\n",
    "\n",
    "print(\"=== Demo: Related Questions ===\")\n",
    "print(\"Note: Each question is independent - no conversation history is maintained in this demo\\n\")\n",
    "\n",
    "questions = [\n",
    "    \"What is Python?explain in few words\",\n",
    "    \"What are the main advantages of this language?\",    \n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = get_ai_response(question)\n",
    "    print(f\"Q: {question}\\nA: {answer}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Conversation with History\n",
    "# This example shows how to maintain conversation context across multiple requests\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-goog-api-key\": gemini_api_key\n",
    "}\n",
    "\n",
    "# Maintain conversation history in Gemini format\n",
    "conversation_contents = []\n",
    "\n",
    "def get_ai_response_with_history(user_message, max_tokens=1500, temperature=0.7):\n",
    "    \"\"\"Function to get AI response while maintaining conversation history\"\"\"\n",
    "    # Add user message to history\n",
    "    conversation_contents.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": user_message}]\n",
    "    })\n",
    "    \n",
    "    payload = {\n",
    "        \"contents\": conversation_contents,  # Send entire conversation history\n",
    "        \"generationConfig\": {\n",
    "            \"maxOutputTokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    response_data = response.json()\n",
    "    \n",
    "    # Extract and store AI response\n",
    "    if 'candidates' in response_data:\n",
    "        ai_reply = response_data['candidates'][0]['content']['parts'][0]['text'].strip()\n",
    "        \n",
    "        # Add assistant response to history\n",
    "        conversation_contents.append({\n",
    "            \"role\": \"model\",  # Gemini uses \"model\" instead of \"assistant\"\n",
    "            \"parts\": [{\"text\": ai_reply}]\n",
    "        })\n",
    "        \n",
    "        return ai_reply\n",
    "    else:\n",
    "        return \"Error: No response generated\"\n",
    "\n",
    "print(\"=== Demo: Conversation with History ===\")\n",
    "print(\"Notice how the AI remembers context from previous messages!\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# First question\n",
    "first_question = \"What is Python? Explain in a few words.\"\n",
    "print(f\"\\nüë§ User: {first_question}\")\n",
    "response1 = get_ai_response_with_history(first_question)\n",
    "print(f\"ü§ñ AI: {response1}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Follow-up question - uses \"it\" referring to Python\n",
    "follow_up = \"What are the main advantages of it?\"\n",
    "print(f\"\\nüë§ User: {follow_up}\")\n",
    "print(\"   ‚ÑπÔ∏è  Notice: 'it' refers to Python from previous context\")\n",
    "response2 = get_ai_response_with_history(follow_up)\n",
    "print(f\"ü§ñ AI: {response2}\")\n",
    "print(\"-\" * 80)\n",
    "'''\n",
    "# Another follow-up - builds on the conversation\n",
    "third_question = \"Which of those advantages makes it best for beginners?\"\n",
    "print(f\"\\nüë§ User: {third_question}\")\n",
    "print(\"   ‚ÑπÔ∏è  Notice: 'those advantages' refers to the previous answer\")\n",
    "response3 = get_ai_response_with_history(third_question)\n",
    "print(f\"ü§ñ AI: {response3}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Key Points:\")\n",
    "print(\"‚Ä¢ Conversation history is maintained in 'conversation_contents' list\")\n",
    "print(\"‚Ä¢ Each turn has 'role' (user/model) and 'parts' with text\")\n",
    "print(\"‚Ä¢ Full history is sent with each request\")\n",
    "print(\"‚Ä¢ AI can reference 'it', 'those', 'that' from previous messages\")  '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f1133e",
   "metadata": {},
   "source": [
    "### Complete Setup: Import Required Libraries\n",
    "\n",
    "Setting up all necessary imports for advanced examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62afc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a55c8",
   "metadata": {},
   "source": [
    "### Configure Google Gemini API Credentials\n",
    "\n",
    "Setting up API credentials and endpoints from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API credentials\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    raise ValueError(\"‚ùå GEMINI_API_KEY not found in .env file\")\n",
    "\n",
    "# Set up the API endpoint\n",
    "base_url = \"https://generativelanguage.googleapis.com/v1beta\"\n",
    "model_name = \"gemini-3-flash-preview\"  # You can change to gemini-2.5-flash, gemini-3-pro-preview, etc.\n",
    "completion_endpoint = f\"{base_url}/models/{model_name}:generateContent\"\n",
    "\n",
    "# Configure headers for API requests\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-goog-api-key\": gemini_api_key\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Gemini API configured successfully!\")\n",
    "print(f\"üîó Endpoint: {completion_endpoint}\")\n",
    "print(f\"ü§ñ Model: {model_name}\")\n",
    "print(f\"üîë API Key: {gemini_api_key[:20]}...\" if gemini_api_key else \"‚ùå No API key found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd8fa9",
   "metadata": {},
   "source": [
    "### Helper Functions: Create Payload and Send Requests\n",
    "\n",
    "These functions standardize how we interact with the Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gemini_payload(user_message, system_instruction=None, temperature=0.7, max_tokens=512, thinking_budget=0):\n",
    "    \"\"\"\n",
    "    Create a standardized payload for Gemini API requests\n",
    "    \n",
    "    Args:\n",
    "        user_message: The user's question or prompt\n",
    "        system_instruction: Optional system instruction to guide model behavior\n",
    "        temperature: Controls randomness (0.0-2.0)\n",
    "        max_tokens: Maximum output tokens\n",
    "        thinking_budget: Limit internal reasoning tokens (0 = disable thoughts)\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"text\": user_message\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxOutputTokens\": max_tokens,\n",
    "            \"topK\": 40,\n",
    "            \"topP\": 0.95\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add system instruction if provided\n",
    "    if system_instruction:\n",
    "        payload[\"systemInstruction\"] = {\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"text\": system_instruction\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    return payload\n",
    "\n",
    "\n",
    "def send_gemini_request(payload, show_full_response=False):\n",
    "    \"\"\"\n",
    "    Send request to Gemini API and extract content from response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(completion_endpoint, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        response_data = response.json()\n",
    "        \n",
    "        if show_full_response:\n",
    "            print(\"üîç Full Response:\")\n",
    "            print(json.dumps(response_data, indent=2))\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Extract content from response\n",
    "        if 'candidates' in response_data:\n",
    "            content = response_data['candidates'][0]['content']['parts'][0]['text']\n",
    "            return content\n",
    "        else:\n",
    "            print(\"‚ùå No content found in response\")\n",
    "            if 'error' in response_data:\n",
    "                print(f\"Error: {response_data['error']}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request failed: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå Failed to parse JSON response: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Helper functions created successfully!\")\n",
    "print(\"üìù Functions available:\")\n",
    "print(\"   - create_gemini_payload(): Create standardized API payloads\")\n",
    "print(\"   - send_gemini_request(): Send requests and extract content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Gemini connection with a simple request\n",
    "test_prompt = \"Hello! Please respond  with 'Gemini connection successful' to confirm the connection is working.\"\n",
    "\n",
    "print(\"üîÑ Testing Gemini API connection...\")\n",
    "payload = create_gemini_payload(test_prompt, temperature=0)\n",
    "response = send_gemini_request(payload)\n",
    "\n",
    "if response:\n",
    "    print(\"‚úÖ Gemini Connection Test Result:\")\n",
    "    print(f\"üì§ Sent: {test_prompt}\")\n",
    "    print(f\"üì• Received: {response}\")\n",
    "else:\n",
    "    print(\"‚ùå Connection test failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78542ed8",
   "metadata": {},
   "source": [
    "## Example: Article Classification Without System Instruction\n",
    "\n",
    "Let's see how the model responds to a classification task without any specific instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample article for testing\n",
    "california_drought_article = \"\"\"\n",
    "Severe drought likely in California\n",
    "\n",
    "Millions of California residents are bracing for less water and dry lawns as drought threatens to leave a large swath of the region with a growing water shortage.\n",
    "\n",
    "In a remarkable indication of drought severity, officials in Southern California have declared a first-of-its-kind action limiting outdoor water use to one day a week for nearly 8 million residents.\n",
    "\n",
    "Much remains to be determined about how daily life will change as people adjust to a drier normal. But officials are warning the situation is dire and could lead to even more severe limits later in the year.\n",
    "\"\"\"\n",
    "\n",
    "# Basic prompt without specific instructions\n",
    "basic_prompt = f\"\"\"What kind of article is this?\n",
    "---\n",
    "{california_drought_article}\"\"\"\n",
    "\n",
    "print(\"üìù Basic Prompt (No System Instruction):\")\n",
    "print(\"=\"*60)\n",
    "print(basic_prompt)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ea64a",
   "metadata": {},
   "source": [
    "## Send Basic Request (No System Instruction)\n",
    "\n",
    "Getting a response without any specific behavioral guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send basic prompt and see response\n",
    "payload = create_gemini_payload(basic_prompt, temperature=0)\n",
    "response = send_gemini_request(payload)\n",
    "\n",
    "print(\"ü§ñ Basic Response (No System Instruction):\")\n",
    "print(\"=\"*60)\n",
    "if response:\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"‚ùå No response received\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9a869",
   "metadata": {},
   "source": [
    "## Example: Article Classification WITH System Instruction\n",
    "\n",
    "Now let's use a system instruction to guide the model's behavior for concise responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0133c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure system instruction for specific behavior\n",
    "system_instruction = \"You are a news aggregator that categorizes news articles. Respond with a single category word only.\"\n",
    "\n",
    "# Same user prompt but with system instruction\n",
    "user_prompt = f\"\"\"What kind of article is this?\n",
    "---\n",
    "{california_drought_article}\"\"\"\n",
    "\n",
    "print(\"üéØ System Instruction:\")\n",
    "print(\"=\"*60)\n",
    "print(system_instruction)\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìù User Prompt:\")\n",
    "print(\"=\"*60)\n",
    "print(user_prompt)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03a419",
   "metadata": {},
   "source": [
    "## Send Request WITH System Instruction\n",
    "\n",
    "Compare the difference when we guide the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request with system instruction\n",
    "payload = create_gemini_payload(user_prompt, system_instruction=system_instruction, temperature=0)\n",
    "response = send_gemini_request(payload)\n",
    "\n",
    "print(\"ü§ñ Response WITH System Instruction:\")\n",
    "print(\"=\"*60)\n",
    "if response:\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"‚ùå No response received\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(\"‚Ä¢ Without system instruction: Likely gives detailed description\")\n",
    "print(\"‚Ä¢ With system instruction: Should give single category word\")\n",
    "print(\"\\nüí° System instructions are powerful for controlling output format and behavior!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
